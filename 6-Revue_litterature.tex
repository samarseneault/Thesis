\Chapter{LITERATURE REVIEW}\label{sec:RevLitt}
This chapter explores relevant state of the art literature related to the domain of this thesis. This includes insights into the following topics: swarm robotics, decentralized storage, communication and consensus, distributed storage, risk assessment in robot swarms, and path planning and exploration strategies. Not all articles explored in this review were directly used, but they certainly all provide good background information. The direct contribution of those which were used for either \ac{RASS} or \ac{DORA} is explained in detail in their respective chapters.

\section{Swarm Robotics}
There has been a growing interest for swarm robotics \cite{dorigo2020reflections,yang2018grand}, showing they have some potential. A good starting point to explore literature relevant to this work is therefore the overview of swarm robotics published by Dorigo et al. \cite{dorigo2021swarm}, which highlights the benefits and challenges related to the field. A key insight is that individual robots sometimes might not be able to perform tasks on their own, such as surveillance of large areas \cite{renzaglia2012multi}. Another scenario is time-sensitive applications like search and rescue, in which the increased speed provided by parallel task execution might be crucial \cite{mcguireMinimalNavigationSolution2019a}. In these cases where multiple robots are required, coordination through a central source might be considered a simpler solution, but it has drawbacks and may be impossible to use at times. This is why decentralized swarm control is often considered, but it too has challenges, like storage, communication and consensus mechanisms \cite{davis2016consensus}. The last problematic is out of the scope of this work, but the other two deserve attention.

\section{Decentralized Storage}
To support collaboration within a swarm, efficient communication is necessary \cite{dutta2020efficient}. To that end, relevant articles supporting distributed or decentralized ways of sharing data are presented here. One of the first accounts of decentralized data transmission in the context of swarm intelligence was proposed in Ant Colony Optimization \cite{dorigo2006ant}, in which agents leave pheromone trails in their environment which can be interpreted by their peers. This mechanism is a form of stigmergy.

Using the same concept to store data, the virtual stigmergy \cite{pinciroliTuple2016} is a distributed key-value storage mechanism that allows a swarm of robots to efficiently share information at runtime, and effectively constitutes a shared memory space. It was designed to work with robots with limited bandwidth, lossy communication mediums, and low processing capabilities. It ensures full data replication across all of its storage nodes (i.e. the robots), meaning it is particularly robust to failures. It also aims for eventual data consistency \footnote{Eventual consistency guarantees that if a given datum is not modified for a certain time, all accesses to it should eventually become consistent \cite{vogels2009eventually}.} to increase data availability and access speed. Another advantage of virtual stigmergy is its inherent integration with Buzz. SwarmMesh \cite{majcherczykSwarmmesh2020}, another distributed key-value storage mechanism, introduces the idea of storing data on the less vulnerable members of the swarm to increase robustness. This is done by calculating keys by hashing the data needing to be stored and then partitioning the keys based on the fitness (connectivity, available memory, etc.) of a given robot to store information. Access to stored information is done through a custom routing algorithm based on the keys. The system also allows for data replication to further improve robustness. Its usefulness has been proven in situations where robot resources and capabilities are limited \cite{majcherczyk2021distributed}. Both virtual stigmergy and SwarmMesh are scalable to large swarms of robots with dynamic connection topologies. However, their main limitation resides mainly in the small size of the data that can be stored in the swarm. This is an issue if larger quantities of data need to be shared. For example, storing pictures, videos or complex data collected during a mission with these systems might not be feasible. This is because they lack a mechanism to split data in chunks to reference and reconstruct stored files. SOUL \cite{varadharajan2020soul} addresses this by adapting existing peer-to-peer file sharing mechanisms (which are already decentralized in nature and support large file sharing) to the context of robotic swarms. The key principle in SOUL is that files are "split into a series of smaller chunks of data, referred to as datagrams, spread throughout the swarm". This spreading is done through a bidding process among the members of the swarm to determine which storage configuration will minimize the cost of reconstructing data (i.e. retrieving it) from the distributed datagrams. It might be necessary to use distributed storage mechanism which are optimized for certain metrics. In \cite{amigoni2017multirobot}, a method for scenarios in which bandwidth is limited is suggested. In situations where the location of the storage nodes is important, geographic hash tables can be used \cite{wu2008ldht,ratnasamy2002ght,ahullo2008supporting}. For other constraints such as requirements for low energy usage, Cell Hash Routing  \cite{araujo2005chr} may be considered as a starting point. It is well suited to networks with dynamic topologies and varying densities. All the previously mentioned storage solutions are part of a wider category of storage designs called \ac{CRDT}, which aim to provide fast data availability and eventual data consistency.

\section{Routing}
To transmit data efficiently between two devices which are not directly connected, a routing strategy is necessary. That type of situation occurs in swarm robotics applications, where robots are often spread over a terrain and do not form a fully connected topology. Finding the best path between two points can be done with Dijkstra's algorithm \cite{dijkstra1959note}. However, such a method requires knowing the network topology in advance, and is quite computationally expensive, in the order of $\Theta((|V|+|E|)\log|V|)$ if $V$ vertices and $E$ edges are involved in the network. It is a prohibitive cost, especially if it needs to be computed periodically in dynamic topologies. Similarly, voting-based routing algorithms cannot be considered, as the voting process might introduce significant delays.

For these reasons, it is worth considering algorithms designed specifically for swarm robotics applications. By far one of the most prevalent approaches is to route data following a gradient based on one metric \cite{draves2004comparison,watteyne2009implementation}. The most used metric, perhaps because of its simplicity and efficiency, is the hop count between a source node and a target node. It is used notably in \cite{watteyne2009implementation,kuruvila2005hop,zhang2014efficient,al2019efficient} for routing in ad hoc networks and in \ac{WSN}s. Designs based on classic swarm intelligence algorithms can be used to find optimal routes, such as \cite{li2011slime,jiang2018toward,jiang2018effective,liao2008data,tolstaya2021learning}, but are more effective in static topologies, which makes their applications somewhat limited. Methods inspired by epidemics were shown to be efficient for disseminating data \cite{ganesan2002empirical,hui2004dynamic}. For data aggregation purposes, which is an application similar to the percolation \ac{RASS} seeks to achieve, the techniques in  \cite{jiang2018effective,liao2008data,dhand2016data} were all shown to provide good results and could thus serve as inspiration.

\section{Risk Assessment}
Many methods to detect failures have been proposed. Endogenous (from internal origin) fault detection aims for robots to detect errors that occur to themselves. These include the like of malfunction of a particular sensor or a disconnection from the \textit{ad hoc} network to which they are connected. Methods to this effect include those proposed by \cite{roumeliotis1998sensor}, which uses Kalman Filters \cite{kalman1960new} to detect outliers in sensor behavior, as well as others based on Bayesian networks \cite{lerner2000bayesian} or particle filters \cite{li2001particle}. Another algorithm inspired by fireflies allows exogenous fault detection, that is sensing failures in other members of the swarm \cite{christensen2009fireflies}. A more recent exogenous (from external origin) anomaly detection solution was proposed in \cite{tarapore2017generic} and takes inspiration from the immune system. Taken separately or even in a possible combination, these fault detection have the potential to improve swarm resilience to failures. However, reaching a state where robots malfunction or crash is simply undesirable.  

While some failures might be purely stochastic and unpredictable, some arise due to unnecessary exposure to risk. Therefore, of particular importance to the research objective of minimizing failures in the designed systems is the consideration of risk, which is directly correlated to breakdowns. Thus, strategies on how to assess the presence of danger and on how to share this information among the members of the swarm are necessary for \ac{RASS} and for \ac{DORA}. The definitions and categorizations of risk suggested in \cite{xiao2020robot} are good starting points for creating risk assertions. These categories are locale-dependent, action-dependent and traverse-dependent. They respectively denote risk that does not depend on previous history (states and actions of the robots), risk that depends on recent history and risk which is linked to the whole history. An example of locale-dependent risk is given in \cite{de2011minimum}, in which local terrain elevation may pose a threat to aircrafts if they fly too close. Traverse-dependent risk would be involved in situations where a cumulative effect is observed, like in prolonged exposition to radiation. These definitions can be useful to model the effects of long term exposure to danger. However, they definitions were created and used in the context of a formal path planner which is too resource-demanding in this current context. Also, this planner assumes prior knowledge of the environment's state, meaning that risk is not discovered by the robots themselves, but rather by a central operator. In SPIDER \cite{hunt2020spider}, agents are tasked with chain formation in dangerous environments. They adopt shy or bold behaviors, which allows them to adapt to varying levels of risk. This behavioral adaptability allows them to be resilient to significant failures and member loss. It is also interesting to note that many risk types can be combined through weighted sums to account for diverse scenarios \cite{soltani2004fuzzy}. Some association may also be made between risk and reward, as some exposure to reasonable amounts of the former may result in significant gains. For this reason, path planners described in \cite{ono2008efficient,vitus2011feedback} allocate a "risk budget" to their agents. In other words, a balance between risk and reward can be used to guide robots through optimization, given that the risk tolerance of the system is properly defined.


\section{Exploration Strategies}
One possible strategy for exploration is based on ant colony stigmergy, in which virtual pheromones are used to dissuade robots from going to certain locations \cite{hunt2019testing}. This mechanism can be used to guide robots away from each other (also know as dispersal) or away from uninteresting areas and therefore to steer exploration. However, the authors report its results are limited in dense topology scenarios.

The second possible strategy for terrain coverage is path planning. The planners suggested by \cite{undurti2010online,thiebaux2016rao,xiao2020robot} are based on a \ac{MDP} and are risk-oriented. Their shortcoming in relation to this work's objective are twofold. First, they assume at least a partial knowledge of the global state of the environment is available. This is not a valid assumption when exploring unknown environments, which  is precisely what is defined in \ac{DORA}'s main functional requirement. Second, they are only applied to single-robot systems, and therefore fall short of the problem elements this works seeks to resolve.

Addressing the last point, several terrain coverage maximization strategies which are distributed in nature have been proposed. Many distributed exploration strategies that maximize the amount of covered terrain have been proposed. Voronoi-based coverage control techniques~\cite{luo2019voronoi,santos2019decentralized} achieve interesting results, but are more useful when prior knowledge of the environment exists. The same issue applies to methods using time-varying domains (i.e. dynamic environments) \cite{santos2019decentralized,xu2019multi}. A perhaps simpler form of exploration which offers optimal terrain coverage (if not total, in the absence of failures), and consequently more suited to \ac{DORA}'s requirements, is \ac{FBE} \cite{yamauchi1998frontier}. In this method, robots build a common knowledge of the furthest state of exploration their environment, and seek to widen this frontier at every time step, eventually resulting in full environment coverage. No prior knowledge is required by the agents. Several \ac{FBE} refinements have been developed: Particle Swarm Optimization \cite{wang2011frontier} and Wavefront Frontier Detector \cite{topiwala2018frontier} are two of them. However, none of \ac{FBE}-based strategies take risk into account. Because of their good performance in terms of terrain coverage, they could be used as benchmark solutions.

The system which is closest to \ac{DORA}'s objectives is the multi-robot control algorithm presented in \cite{dames2012decentralized,schwagerMultirobotControlPolicy2017}, because it maximizes the information gain during exploration in the presence of unknown hazards. Yet, it is not perfectly aligned with all non-functional requirements because it seeks an optimal exploration solution, which entails a computational complexity greater than $O(|V|2^{|V|})$ for every time step, where $V$ is the set of robots. Using it would require introducing several approximations to lower the computation load of the robots.


\section{Swarm Programming and Simulation}
Swarm programming focuses on four key properties: decentralized control, absence of leaders, absence of predefined roles, and reliance on simple and local interactions. In this sense, it differs greatly from conventional centralized cloud-based applications or even from centralized sensor arrays which are common in \ac{IoT} applications. Many programming paradigms for robot swarms have their shortcomings to address the issue of coordinating swarms of robots. This is the case for robot oriented programming, which is embodied by ROS, \cite{quigley2009ros}, because it focuses on single-robot scenarios. Also, aggregate programming like Protelis \cite{pianini2015protelis} suffers from offering little control over individual robot behavior. Granted, swarm robotics should avoid behavior specialization, but sometimes it might be necessary, for example when hierarchies are needed or when location-specific tasks must be completed. For task-oriented programming frameworks such as Voltron \cite{mottola2014team}, the issue is that they allow little control over low-level operations which might be required when operating robots.

Buzz \cite{pinciroliBuzz2016}, another programming language, addresses these issues and aims to meet the key properties mentioned earlier, therefore enabling an easier approach to swarm programming. Among other things, Buzz uses a custom virtual machine to run code in isolation of the operating system. It is designed to be an extensible language with high-level primitives facilitating robot interactions. A recently developed language, Koord \cite{ghosh2020koord}, could also inspire software testing mechanisms, as it is built around a strong formal method philosophy. This means it allows swarms of robots to be programmed with verifiability in mind, that is each part of the distributed algorithms should be tested in a modular way with fixed guarantees in place.

Because validating various aspects of the proposed systems requires simulations, it is necessary to use a simulator well suited to scenarios in which multiple agents are present. It is also necessary to mitigate the previously mentioned robot-simulation gap. In this regard, ARGoS  \cite{Pinciroli:SI2012} is particularly well suited, because it has the capacity to run agents on separate threads, therefore improving simulation execution speed. This has the effect of making large scale simulations accessible. Moreover, because ARGoS is a physics-based simulator, it allows to take into account the effects of physical interactions between robots, their environment and themselves. Furthermore, ARGoS integrates particularly well with Buzz, with the possibility of directly using compiled Buzz scripts.

\section{Grid Mapping}
In the task of map building which is required for (or is the purpose of) robotic exploration \cite{gutmann1999incremental,hahnel2002map}, belief maps offer a simple and powerful tool. They show significant improvements for exploration performance over occupancy maps because of their increased precision which allows for more informed decisions \cite{stachnissMappingExplorationMobile2003}. Their use in robotic exploration is far from new \cite{kobayashiSharingExploringInformation2002,kobayashiDeterminationExplorationTarget2003}. In these, robots share sensor measurements and then build and update the belief maps independently. The disadvantages of the three previous methods are that they are limited to fixed grid sizes and are tested only with two robots, which make their application limited. Recently, in \cite{indelmanCooperativeMultirobotBelief2018}, improvements for multi-robot grid mapping have been suggested. In that article, the robots consider both the current and expected beliefs to collaborate. However, there seems to be no usage of them for storing risk beliefs. These maps are well suited to be stored in \ac{CRDT}s, because cell locations can be used as keys and beliefs as values.